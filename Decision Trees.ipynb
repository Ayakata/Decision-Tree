{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# <center>Решающее дерево (Decision Tree)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata = pd.read_csv(\"_ea07570741a3ec966e284208f588e50e_titanic.csv\", index_col=\"PassengerId\")\n",
    "rdata = rdata.reindex(columns=[\"Sex\", \"Age\", \"Pclass\", \"Fare\", \"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdata.loc[rdata[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "rdata.loc[rdata[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "rdata = rdata.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Для начала организуем стандартную функцию разбиения на `train` и `test`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первым аргументом она будет принимать датасет\n",
    "# Вторым долю тестовой выборки от общей типа float, по умолчанию выставим 0.1\n",
    "\n",
    "def train_test_split(data, ratio:float=0.1):\n",
    "    \n",
    "    # отберём индексы по количеству \n",
    "    ids = pd.DataFrame(data).index.tolist()\n",
    "    \n",
    "    # cгенерируем уникальные случайноые индексы из списка в заданном процентном соотношении\n",
    "    ids = np.random.choice(ids, round(len(ids)*ratio), replace=False)\n",
    "    \n",
    "    # выберем строки с полученными индексами для тестовой выборки\n",
    "    # исключим эти строки для тренировочной\n",
    "    test = pd.DataFrame(data).loc[ids]\n",
    "    train = pd.DataFrame(data).drop(ids)\n",
    "    \n",
    "    return np.array(train), np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее нам необходим функция, которая будет определять один ли класс остался\n",
    "# в подвыборке или же нужно продолжать разбивать. Тут всё банально\n",
    "# Если количество уникальных классов > 1 то возвращаем False\n",
    "# Подразумевается, что принадлежность классу всегда в последней колонке\n",
    "\n",
    "def is_one_class(data):\n",
    "    if len(np.unique(data[:, -1])) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим функцию листа дерева, которая будет классифицировать\n",
    "# окончательно обработанные данные и возрщать их класс\n",
    "\n",
    "# Так же стоит учесть заранее, что в данную подвыбоку могут попасть объекты разных классов,\n",
    "# которые больше невозможно разделить по каким либо признакам\n",
    "# Поэтому просто отнесём все объекты к самому распространённому классу в подвыборке\n",
    "# при помощь функции argmax\n",
    "\n",
    "def class_leaf(data):\n",
    "    \n",
    "    # В первой переменной будут находиться все уникальные классы\n",
    "    # Во второй - как часто они они встречаются в подвыборке\n",
    "    uniq_classes, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    \n",
    "    # Запомним индекс мажоритарного класса\n",
    "    idx = np.argmax(uniq_classes_counts)\n",
    "    \n",
    "    # И вернём класс под этим индексом\n",
    "    return uniq_classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим возможные пороги, по которым ещё можно разбить\n",
    "# наши столбцы с признаками. Задача вернуть словарь вида:\n",
    "# номер признака :  возможные пороги для разбиения\n",
    "\n",
    "def get_thresholds(data):\n",
    "    thresholds = {}\n",
    "    \n",
    "    # Определим количесво фичей (столбцов)\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # И для каждой фичи (исключая таргет)\n",
    "    # Определим все уникальные значения\n",
    "    for n_feature in range(n_features - 1):\n",
    "        thresholds[n_feature] = []\n",
    "        unique_values = np.unique(data[:, n_feature])\n",
    "        \n",
    "        # Теперь по всему столбцу признаков определим порог\n",
    "        # (среднее), по которому потенциально можно будет разделить выборку\n",
    "        for idx in range(len(unique_values)):\n",
    "            if idx:\n",
    "                cur_value = unique_values[idx]\n",
    "                prev_value = unique_values[idx - 1]\n",
    "                possible_split = (cur_value + prev_value) / 2\n",
    "                thresholds[n_feature].append(possible_split)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь нам понадобится функция разбиения подвыборки на ветви\n",
    "# по данному порогу и столбцу\n",
    "\n",
    "def split_by_class(data, column, thresh):\n",
    "    \n",
    "    left_branch = data[data[:, column] <= thresh]\n",
    "    right_branch = data[data[:, column] > thresh]\n",
    "    \n",
    "    return left_branch, right_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----------\n",
    "     Для определения наиболее оптимального порога для разбиения подвыборки используется несколько критериев информативности.\n",
    "    Рассмотрим наиболее часто используемые:\n",
    "    \n",
    "    Подробнее о них:\n",
    "   [КРИТЕРИЙ ИНФОРМАТИВНОСТИ](http://www.machinelearning.ru/wiki/images/8/89/Sem3_trees.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## Энтропийный критерий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    В итоге нам нужно найти минимальную энтропию при всех возможных распределениях подвыборки по ветвям \n",
    "    (по всем столбцам и порогам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "#### Энтропия по подвыборке\n",
    "## $$ H(p) = \\sum_{i=1}^cp_i  (-log_2 p_i)$$\n",
    "\n",
    "--------------\n",
    "\n",
    "#### Энтропийный критерий информативности\n",
    "\n",
    "## $$ Q_H = \\frac{N_{l}}{N_m}H(p_{l}) +  \\frac{N_{r}}{N_m}H(p_{r}) \\rightarrow min$$\n",
    "\n",
    "Где <b>_l_</b> - подвыборка из левой ветви, а <b>_r_</b> - подвыборка из правой ветви\n",
    "\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем энтропию в подвыборке\n",
    "\n",
    "def get_entropy(data):\n",
    "    \n",
    "    # для начала, определим долю каждого класса в выборке\n",
    "    # нам понадобится общее количесво объектов каждого класса в ней\n",
    "    _, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    \n",
    "    # разделим количество каждого класса на общее их число и получим долю каждого класса\n",
    "    rate = uniq_classes_counts / uniq_classes_counts.sum()\n",
    "    \n",
    "    # теперь по формуле может посчитать энтропию в этой подвыборке\n",
    "    entropy = sum(rate * -np.log2(rate))\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(data):\n",
    "    _, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    rate = uniq_classes_counts / uniq_classes_counts.sum()\n",
    "    gini = 1 - sum(rate ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь можно задать функцию подсчёта энтропии по двум ветвям при определённом распределении\n",
    "\n",
    "def get_overall_entropy(left_branch, right_branch):\n",
    "    \n",
    "    n_all = len(left_branch) + len(right_branch)\n",
    "    n_left = len(left_branch) / n_all\n",
    "    n_right = len(right_branch) / n_all\n",
    "    \n",
    "    # так же по следующей формуле считаем суммарную энтропию на общую долю\n",
    "    overall_entropy = (n_left * get_entropy(left_branch) + n_right * get_entropy(right_branch))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И так, у нас всё готово для поиска критерия информативности\n",
    "# через подбор оптимальной (минимальной) энтропии\n",
    "\n",
    "def impurity_function(data, thresholds):\n",
    "    \n",
    "    overall_entropy = 666\n",
    "    \n",
    "    # для каждого стоблца в словаре со всеми возможными порогами\n",
    "    for col in thresholds:\n",
    "        # для каждого порога в нём\n",
    "        for thresh in thresholds[col]:\n",
    "            # разобьём подвыборку на ветви и подсчтаем энтропию для каждого разбиения\n",
    "            left_branch, right_branch = split_by_class(data, column=col, thresh=thresh)\n",
    "            i_entropy = get_overall_entropy(left_branch, right_branch)\n",
    "            \n",
    "            # и если текущее значение энтропии меньше имеющегося, то обновляем значения\n",
    "            if i_entropy <= overall_entropy:\n",
    "                overall_entropy = i_entropy\n",
    "                optimal_col = col\n",
    "                optimal_thresh = thresh\n",
    "    \n",
    "    return optimal_col, optimal_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пришло время обернуть всё в рекурсивный алгоримт дерева,\n",
    "# который будет иметь вид словаря из словарей, представляющего наше дерево\n",
    "\n",
    "def tree(data, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    # крайний случай (если находимся в терминальной ветке)\n",
    "    # тогда вернём предсказание класса в этой ветке\n",
    "    if is_one_class(data) or len(data) < min_samples or counter > max_depth:\n",
    "        return class_leaf(data)\n",
    "    \n",
    "    # иначе рекурсия ...\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # разобьём подвыборку на ветви по критерию информативности\n",
    "    thresholds = get_thresholds(data)\n",
    "    \n",
    "    # если нет порогов для разбиения классов - предскажем класс\n",
    "    is_thresholds = 0\n",
    "    for i in thresholds:\n",
    "        is_thresholds += len(thresholds[i])\n",
    "    if not is_thresholds:\n",
    "        return class_leaf(data)\n",
    "    \n",
    "    optimal_column, optimal_threshold = impurity_function(data, thresholds)\n",
    "    left_branch, right_branch = split_by_class(data, optimal_column, optimal_threshold)\n",
    "    \n",
    "    # определим, как будет выглядеть наш словарь с вопросами-ответами\n",
    "    # обозначим условие разбиения\n",
    "    condition = \"{} <= {}\".format(optimal_column, optimal_threshold)\n",
    "    sub_tree = {condition: []}\n",
    "    \n",
    "    # разбиваем на ветви дальше пока не классифицируем все объекты\n",
    "    to_left = tree(left_branch, counter, min_samples, max_depth)\n",
    "    to_right = tree(right_branch, counter, min_samples, max_depth)\n",
    "    \n",
    "    if to_left == to_right:\n",
    "        sub_tree = to_left\n",
    "    else:\n",
    "        sub_tree[condition].append(to_left)\n",
    "        sub_tree[condition].append(to_right)\n",
    "    \n",
    "    return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "derevo = tree(rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0 <= 0.5': [{'2 <= 2.5': [{'3 <= 28.85625': [{'3 <= 28.23125': [1, 0]},\n",
       "      {'1 <= 2.5': [0,\n",
       "        {'3 <= 149.0354': [1, {'3 <= 152.50625000000002': [0, 1]}]}]}]},\n",
       "    {'3 <= 20.799999999999997': [{'1 <= 16.5': [1,\n",
       "        {'1 <= 36.5': [{'3 <= 18.62915': [0, 1]}, {'1 <= 55.0': [0, 1]}]}]},\n",
       "      {'1 <= 5.5': [{'1 <= 3.5': [0, 1]},\n",
       "        {'3 <= 31.331249999999997': [0, {'3 <= 32.88125': [1, 0]}]}]}]}]},\n",
       "  {'2 <= 1.5': [{'1 <= 53.0': [{'3 <= 25.9375': [0,\n",
       "        {'3 <= 27.1354': [1, {'1 <= 17.5': [1, 0]}]}]},\n",
       "      {'1 <= 75.5': [{'3 <= 35.0771': [0, {'3 <= 42.5021': [1, 0]}]}, 1]}]},\n",
       "    {'1 <= 9.5': [{'3 <= 20.825': [1, {'2 <= 2.5': [1, 0]}]}, 0]}]}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name, _, val = list(derevo.keys())[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata = pd.read_csv(\"_ea07570741a3ec966e284208f588e50e_titanic.csv\", index_col=\"PassengerId\")\n",
    "ldata = ldata.reindex(columns=[\"Sex\", \"Age\", \"Pclass\", \"Fare\", \"Survived\"]).dropna()\n",
    "ldata.loc[ldata[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "ldata.loc[ldata[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "ldata = ldata.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldata[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь на базе сформированного дерева можем делать предсказание\n",
    "\n",
    "def predict(obj, tree):\n",
    "    \n",
    "    # представим первое условие в поддереве в виде списка\n",
    "    # и разобьём на индекс признака и значение порога\n",
    "    condition = list(tree.keys())[0]\n",
    "    f_name, _, val = condition.split()\n",
    "    \n",
    "    # если условие выполняется то ответ в левой части дерева иначе в правой\n",
    "    if obj[int(f_name)] <= float(val):\n",
    "        answer = tree[condition][0]\n",
    "    else:\n",
    "        answer = tree[condition][1]\n",
    "        \n",
    "    # если ответом является классификация в терминальной ветке, а не словарь, то возвращаем его\n",
    "    # иначе отправляемся дальше по словарю\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        return predict(obj, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data, tree):\n",
    "    data = pd.DataFrame(data)\n",
    "    target = data.iloc[:, -1]\n",
    "    data['classify'] = data.apply(predict, axis=1, args=(tree,))\n",
    "    data['correct'] = data.classify == target\n",
    "    return data.correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8451178451178452"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(rdata, derevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
