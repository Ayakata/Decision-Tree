{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "# <center>Решающее дерево (Decision Tree)</center>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Решающее дерево - это не тот самый ясень, у которого можно спросить за любимую, а алгоритм классификации (так же приспособлен и для регрессии), представляющий из себя циклический граф с двумя типами вершин: первая соединена с двумя дочерними и отправляет объекты из выборки в одну из них в зависимости от того - удовлетворяет ли объект условию в вершине или нет, вторая не соединена ни с чем и лишь даёт ответ на основе собранных данных из предыдущих вершин. И спросить у такого дерева можно, пожалуй что угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hi](cate.jpg \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Но чтобы дерево могло предсказывать что-то его нужно в начале обучить. Для этого реализуем такой алгоритм и разберём, как он работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from matplotlib import pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Будем обучать решающее дерево на датасете из соревнования на В нём представлены данные о пассажирах Титаника и требуется, на основе этих данных сделать предсказание о том - \n",
    "    выжил ли пассажир или нет.\n",
    "    По хорошему, нужно проводить подробный анализ и на его основе модифицировать данные\n",
    "    и затем только обучать дерево (и не одно), но сейчас это не требуется.\n",
    "    Отберём сразу лишь ключевые признаки. Этого уже хватит, чтобы получить приличную точность предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрзим данные и оставим лишь ключевые колонки с признаками:\n",
    "# пол, возраст, номер класса, стоимость билета, и колонка с таргетом (выжил или нет пассажир)\n",
    "rdata = pd.read_csv(\"_ea07570741a3ec966e284208f588e50e_titanic.csv\", index_col=\"PassengerId\")\n",
    "\n",
    "# по хорошему, на основе анализа пустые значения лучше заполнять на основе анализа, но мы их просто отбросим\n",
    "rdata = rdata.reindex(columns=[\"Sex\", \"Age\", \"Pclass\", \"Fare\", \"Survived\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим категориальный признак пола на числа 1 и 0\n",
    "# Наше дерево пока не умеет работать с текстовыми данными\n",
    "\n",
    "rdata.loc[rdata[\"Sex\"] == \"male\", \"Sex\"] = 1\n",
    "rdata.loc[rdata[\"Sex\"] == \"female\", \"Sex\"] = 0\n",
    "rdata = rdata.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Для начала организуем стандартную функцию разбиения на `train` и `test`</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Первым аргументом она будет принимать датасет\n",
    "# Вторым долю тестовой выборки от общей типа float, по умолчанию выставим 0.1\n",
    "\n",
    "def train_test_split(data, ratio:float=0.1):\n",
    "    \n",
    "    # отберём индексы по количеству \n",
    "    ids = pd.DataFrame(data).index.tolist()\n",
    "    \n",
    "    # cгенерируем уникальные случайноые индексы из списка в заданном процентном соотношении\n",
    "    ids = np.random.choice(ids, round(len(ids)*ratio), replace=False)\n",
    "    \n",
    "    # выберем строки с полученными индексами для тестовой выборки\n",
    "    # исключим эти строки для тренировочной\n",
    "    test = pd.DataFrame(data).loc[ids]\n",
    "    train = pd.DataFrame(data).drop(ids)\n",
    "    \n",
    "    return np.array(train), np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Далее нам необходим функция, которая будет определять один ли класс остался\n",
    "# в подвыборке или же нужно продолжать разбивать. Тут всё банально\n",
    "# Если количество уникальных классов > 1 то возвращаем False\n",
    "# Подразумевается, что принадлежность классу всегда в последней колонке\n",
    "\n",
    "def is_one_class(data):\n",
    "    if len(np.unique(data[:, -1])) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим функцию листа дерева, которая будет классифицировать\n",
    "# окончательно обработанные данные и возрщать их класс\n",
    "\n",
    "# Так же стоит учесть заранее, что в данную подвыбоку могут попасть объекты разных классов,\n",
    "# которые больше невозможно разделить по каким либо признакам\n",
    "# Поэтому просто отнесём все объекты к самому распространённому классу в подвыборке\n",
    "# при помощь функции argmax\n",
    "\n",
    "def class_leaf(data):\n",
    "    \n",
    "    # В первой переменной будут находиться все уникальные классы\n",
    "    # Во второй - как часто они они встречаются в подвыборке\n",
    "    uniq_classes, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    \n",
    "    # Запомним индекс мажоритарного класса\n",
    "    idx = np.argmax(uniq_classes_counts)\n",
    "    \n",
    "    # И вернём класс под этим индексом\n",
    "    return uniq_classes[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим возможные пороги, по которым ещё можно разбить\n",
    "# наши столбцы с признаками. Задача вернуть словарь вида:\n",
    "# номер признака :  возможные пороги для разбиения\n",
    "\n",
    "def get_thresholds(data):\n",
    "    thresholds = {}\n",
    "    \n",
    "    # Определим количесво фичей (столбцов)\n",
    "    n_features = data.shape[1]\n",
    "    \n",
    "    # И для каждой фичи (исключая таргет)\n",
    "    # Определим все уникальные значения\n",
    "    for n_feature in range(n_features - 1):\n",
    "        thresholds[n_feature] = []\n",
    "        unique_values = np.unique(data[:, n_feature])\n",
    "        \n",
    "        # Теперь по всему столбцу признаков определим порог\n",
    "        # (среднее), по которому потенциально можно будет разделить выборку\n",
    "        for idx in range(len(unique_values)):\n",
    "            if idx:\n",
    "                cur_value = unique_values[idx]\n",
    "                prev_value = unique_values[idx - 1]\n",
    "                possible_split = (cur_value + prev_value) / 2\n",
    "                thresholds[n_feature].append(possible_split)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь нам понадобится функция разбиения подвыборки на ветви\n",
    "# по данному порогу и столбцу\n",
    "\n",
    "def split_by_class(data, column, thresh):\n",
    "    \n",
    "    left_branch = data[data[:, column] <= thresh]\n",
    "    right_branch = data[data[:, column] > thresh]\n",
    "    \n",
    "    return left_branch, right_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----------\n",
    "     Для определения наиболее оптимального порога для разбиения подвыборки используется несколько критериев информативности.\n",
    "    Рассмотрим наиболее часто используемые:\n",
    "    \n",
    "    Подробнее о них:\n",
    "   [КРИТЕРИЙ ИНФОРМАТИВНОСТИ](http://www.machinelearning.ru/wiki/images/8/89/Sem3_trees.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "## Энтропийный критерий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    В итоге нам нужно найти минимальную энтропию при всех возможных распределениях подвыборки по ветвям \n",
    "    (по всем столбцам и порогам)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "#### Энтропия по подвыборке\n",
    "## $$ H(p) = \\sum_{i=1}^cp_i  (-log_2 p_i)$$\n",
    "\n",
    "--------------\n",
    "\n",
    "#### Энтропийный критерий информативности\n",
    "\n",
    "## $$ Q_H = \\frac{N_{l}}{N_m}H(p_{l}) +  \\frac{N_{r}}{N_m}H(p_{r}) \\rightarrow min$$\n",
    "\n",
    "Где <b>_l_</b> - подвыборка из левой ветви, а <b>_r_</b> - подвыборка из правой ветви\n",
    "\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем энтропию в подвыборке\n",
    "\n",
    "def get_entropy(data):\n",
    "    \n",
    "    # для начала, определим долю каждого класса в выборке\n",
    "    # нам понадобится общее количесво объектов каждого класса в ней\n",
    "    _, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    \n",
    "    # разделим количество каждого класса на общее их число и получим долю каждого класса\n",
    "    rate = uniq_classes_counts / uniq_classes_counts.sum()\n",
    "    \n",
    "    # теперь по формуле может посчитать энтропию в этой подвыборке\n",
    "    entropy = sum(rate * -np.log2(rate))\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(data):\n",
    "    _, uniq_classes_counts = np.unique(data[:, -1], return_counts=True)\n",
    "    rate = uniq_classes_counts / uniq_classes_counts.sum()\n",
    "    gini = 1 - sum(rate ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь можно задать функцию подсчёта энтропии по двум ветвям при определённом распределении\n",
    "\n",
    "def get_overall_entropy(left_branch, right_branch):\n",
    "    \n",
    "    n_all = len(left_branch) + len(right_branch)\n",
    "    n_left = len(left_branch) / n_all\n",
    "    n_right = len(right_branch) / n_all\n",
    "    \n",
    "    # так же по следующей формуле считаем суммарную энтропию на общую долю\n",
    "    overall_entropy = (n_left * get_entropy(left_branch) + n_right * get_entropy(right_branch))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# И так, у нас всё готово для поиска критерия информативности\n",
    "# через подбор оптимальной (минимальной) энтропии\n",
    "\n",
    "def impurity_function(data, thresholds):\n",
    "    \n",
    "    overall_entropy = 666\n",
    "    \n",
    "    # для каждого стоблца в словаре со всеми возможными порогами\n",
    "    for col in thresholds:\n",
    "        # для каждого порога в нём\n",
    "        for thresh in thresholds[col]:\n",
    "            # разобьём подвыборку на ветви и подсчтаем энтропию для каждого разбиения\n",
    "            left_branch, right_branch = split_by_class(data, column=col, thresh=thresh)\n",
    "            i_entropy = get_overall_entropy(left_branch, right_branch)\n",
    "            \n",
    "            # и если текущее значение энтропии меньше имеющегося, то обновляем значения\n",
    "            if i_entropy <= overall_entropy:\n",
    "                overall_entropy = i_entropy\n",
    "                optimal_col = col\n",
    "                optimal_thresh = thresh\n",
    "    \n",
    "    return optimal_col, optimal_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пришло время обернуть всё в рекурсивный алгоримт дерева,\n",
    "# который будет иметь вид словаря из словарей, представляющего наше дерево\n",
    "\n",
    "def tree(data, counter=0, min_samples=2, max_depth=5):\n",
    "    \n",
    "    # крайний случай (если находимся в терминальной ветке)\n",
    "    # тогда вернём предсказание класса в этой ветке\n",
    "    if is_one_class(data) or len(data) < min_samples or counter > max_depth:\n",
    "        return class_leaf(data)\n",
    "    \n",
    "    # иначе рекурсия ...\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # разобьём подвыборку на ветви по критерию информативности\n",
    "    thresholds = get_thresholds(data)\n",
    "    \n",
    "    # если нет порогов для разбиения классов - предскажем класс\n",
    "    is_thresholds = 0\n",
    "    for i in thresholds:\n",
    "        is_thresholds += len(thresholds[i])\n",
    "    if not is_thresholds:\n",
    "        return class_leaf(data)\n",
    "    \n",
    "    optimal_column, optimal_threshold = impurity_function(data, thresholds)\n",
    "    left_branch, right_branch = split_by_class(data, optimal_column, optimal_threshold)\n",
    "    \n",
    "    # определим, как будет выглядеть наш словарь с вопросами-ответами\n",
    "    # обозначим условие разбиения\n",
    "    condition = \"{} <= {}\".format(optimal_column, optimal_threshold)\n",
    "    sub_tree = {condition: []}\n",
    "    \n",
    "    # разбиваем на ветви дальше пока не классифицируем все объекты\n",
    "    to_left = tree(left_branch, counter, min_samples, max_depth)\n",
    "    to_right = tree(right_branch, counter, min_samples, max_depth)\n",
    "    \n",
    "    if to_left == to_right:\n",
    "        sub_tree = to_left\n",
    "    else:\n",
    "        sub_tree[condition].append(to_left)\n",
    "        sub_tree[condition].append(to_right)\n",
    "    \n",
    "    return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN, TEST = train_test_split(rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "derevo = tree(TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь на базе сформированного дерева можем делать предсказание\n",
    "\n",
    "def predict(obj, tree):\n",
    "    \n",
    "    # представим первое условие в поддереве в виде списка\n",
    "    # и разобьём на индекс признака и значение порога\n",
    "    condition = list(tree.keys())[0]\n",
    "    f_name, _, val = condition.split()\n",
    "    \n",
    "    # если условие выполняется то ответ в левой части дерева иначе в правой\n",
    "    if obj[int(f_name)] <= float(val):\n",
    "        answer = tree[condition][0]\n",
    "    else:\n",
    "        answer = tree[condition][1]\n",
    "        \n",
    "    # если ответом является классификация в терминальной ветке, а не словарь, то возвращаем его\n",
    "    # иначе отправляемся дальше по словарю\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "    else:\n",
    "        return predict(obj, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для оценки точности предсказаний\n",
    "# она будет принимать в качестве аргумента тестовую выборку и дерево\n",
    "\n",
    "def accuracy(data, tree):\n",
    "    data = pd.DataFrame(data)\n",
    "    target = data.iloc[:, -1]\n",
    "    data['classify'] = data.apply(predict, axis=1, args=(tree,))\n",
    "    data['correct'] = data.classify == target\n",
    "    return data.correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8309859154929577"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(TEST, derevo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "83% Но можно и куда лучше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
